


























#Imports

import numpy as np
import random
import matplotlib.pyplot as plt
import random


recipes = []
recipes.append([0,1,2,0,1,2])
recipes.append([0,1,1,0,1,1])
recipes.append([0,1,1,0,2])
recipes.append([0,2,0,2,0,2])
recipes.append([0,1,1,1,1,1])





"""This is the environment for the pancake problem. 
The goal is to create a pancake with the same ingredients as the intended goal."""

class IngredientsActionSpace:
    def __init__(self, n):
        self.n = n  # Number of possible ingredients

    def sample(self): #sample an action
        return random.randint(0, self.n-1) 
    
    def __len__(self):
        return self.n

class PancakeEnv:
    def __init__(self, goals, max_pancake_size=6, partial_reward=False):
        #initialise the environment
        self.goals = goals #list of goals
        self.max_pancake_size = max_pancake_size #maximum number of ingredients in a pancake (including the pancake itself)
        self.action_space = IngredientsActionSpace(3) #0 is the pancake, 1 is the bacon, 2 is the sauce
        self.observation_space() #create the observation space
        self.partial_reward = partial_reward #if we want to give a partial reward


    def reset(self, goal = None): #reset the environment, choose a random goal if none is given
        self.word_for_goal = random.randint(0, len(self.goals)-1) if goal is None else goal #choose a random goal (note that we pass an index, not the actual goal pancake!)
        self.intended_goal = self.goals[self.word_for_goal] #get the actual goal pancake (we won't give it to the agent, but use it for computing the reward)
        self.current_pancake = [0] #we always start with a pancake with no ingredients
        return self.word_for_goal, self.current_pancake

    def step(self, action): #act 1 step in the environment
        self.current_pancake.append(action) #add the ingredient to the pancake
        reward, done = self.compute_reward_and_done() #compute the reward and if the episode is done
        return self.current_pancake, reward, done
        
    def compute_reward_and_done(self):
        if self.current_pancake == self.intended_goal: #we have reached the goal
                return 1, True
        if len(self.current_pancake) == self.max_pancake_size: # we have reached the maximum number of steps
            #if we want to give a partial reward, we compute it here, otherwise we return 0 (failure)
            return 0 if not self.partial_reward else partial_reward(self.intended_goal, self.current_pancake), True
        return 0, False #we are not done yet
        
    
    def observation_space(self):
        # We have all the permutations of pancakes ingredients, from a length of 1 to max_pancake_size. list them all
        # e.g. for max_pancake_size=3, we have
        # [0] [0,0] [0,1] [0,2] [0,0,0] [0,0,1] [0,0,2], [0,1,0]...
        self.observation_space = []
        for i in range(1, self.max_pancake_size+1):
            self.observation_space += self.permutations(i)
    
    def permutations(self, n): #recursive function to get all the permutations of pancakes ingredients
        if n == 1:
            return [[0]]
        perms = []
        for perm in self.permutations(n-1):
            for i in range(3):
                perms.append(perm + [i])
        return perms
    
    def get_observation_index(self, current_pancake): #get the index of the current pancake in the observation space (used for indexing the Q-table)
        return self.observation_space.index(current_pancake)
    
def partial_reward(goal, current_pancake):
    # we give a partial reward based on the number of ingredients that at the right place
    reward = 0
    for i in range(len(goal)):
        if goal[i] == current_pancake[i]:
            reward += 1
    return reward/len(goal)






env = PancakeEnv(recipes, max_pancake_size=6, partial_reward=False)
env.reset()





print("Observation space size: ", len(env.observation_space))
print("Goal space size: ", len(env.goals))
print("Action space size: ", len(env.action_space))
goal_index, initial_observation = env.reset()
print("Sample observation: ", initial_observation)





starting_goal, starting_observation = env.reset()

print("Starting goal: ", starting_goal)
print("Starting Observation: ", starting_observation)





def q_table(): #initialise the Q-table
  goals = len(env.goals)  
  observation_space = len(env.observation_space)  
  action_space = len(env.action_space)  
  Qtable = np.zeros((goals, observation_space, action_space))
  return Qtable
    
def greedy_policy(Qtable, goal_index, state_index): #choose the action with the highest Q-value
  q_values = Qtable[goal_index, state_index]
  action = np.argmax(q_values)
  return int(action)

def epsilon_greedy_policy(Qtable, goal_index, state_index, epsilon): #choose a random action with probability epsilon, otherwise choose the greedy action
    q_values = Qtable[goal_index, state_index]

    if np.random.rand() < epsilon:
        action = random.choice(range(len(q_values)))
    else:
        action = greedy_policy(Qtable, goal_index, state_index)
    return int(action)
    
def random_policy(): 
    return env.action_space.sample()
    





from tqdm.notebook import trange, tqdm
# Training parameters
n_training_episodes = 500000  # Total training episodes
max_steps = 5 # DO NOT CHANGE: Max steps per episode (ingredients you can place)
learning_rate = 0.3          # Learning rate

# Environment parameters
gamma = 0.90               # Discounting rate






"Optional: implement a function to decay epsilon over time and plot it"

def epsilon_decay(episode, n_training_episodes):
    epsilon = max(0.1, 1 - (episode / n_training_episodes))
    return epsilon


def training_agent(n_training_episodes, max_steps, learning_rate, gamma, partial_reward=False, policy=True):
    env = PancakeEnv(recipes, max_pancake_size=6, partial_reward=partial_reward)
    rewards = []
    Qtable = q_table()
    for episode in trange(n_training_episodes):
        goal, state = env.reset()
        done = False
        for step_n in range(max_steps):
            state_idx = env.get_observation_index(state)
            epsilon = epsilon_decay(episode, n_training_episodes)
            if policy == True:
                action = epsilon_greedy_policy(Qtable, goal, state_idx, epsilon)
            else:
                action = random_policy()
            new_state, reward, done = env.step(action)
            new_state_idx = env.get_observation_index(new_state)
            max_Q = np.max(Qtable[goal, new_state_idx])
            Qtable[goal, state_idx, action] = Qtable[goal, state_idx, action] + learning_rate * (reward + gamma * max_Q - Qtable[goal, state_idx, action])
            state = new_state
            if done:
                rewards.append(reward)
                #if episode % 10000 == 0:
                    #print(f"Episode {episode}, reward {reward}")
                break
    return rewards 

rewards_without_partial_reward = training_agent(n_training_episodes, max_steps, learning_rate, gamma, partial_reward=False, policy=True)


"Plot the rewards - depending on your exploration strategy, the curve might be hard to interpret," 
"try using a moving average to smooth it out" 

def moving_average(x, window_size=None):
    if window_size is None:
        window_size = len(x)//10
    return np.convolve(x, np.ones(window_size), 'valid') / window_size

plt.plot(moving_average(rewards_without_partial_reward, 1000), label='Moving Average')
plt.xlabel('Episodes')
plt.ylabel('Reward')
#plt.plot(rewards, label='Reward')
plt.legend()
plt.show()





# Training parameters
n_training_episodes = 500000  # Total training episodes
max_steps = 5 # DO NOT CHANGE: Max steps per episode (ingredients you can place)
learning_rate = 0.3          # Learning rate
# Environment parameters
gamma = 0.8              # Discounting rate


rewards_with_partial_reward = training_agent(n_training_episodes, max_steps, learning_rate, gamma, partial_reward=True, policy=True)


# Training parameters
n_training_episodes = 500000  # Total training episodes
max_steps = 5 # DO NOT CHANGE: Max steps per episode (ingredients you can place)
learning_rate = 0.3          # Learning rate
# Environment parameters
gamma = 0.8              # Discounting rate

rewards_random_policy_partial_reward = training_agent(n_training_episodes, max_steps, learning_rate, gamma, partial_reward=True, policy=False)
rewards_random_policy_NoPArtial_reward = training_agent(n_training_episodes, max_steps, learning_rate, gamma, partial_reward=False, policy=False)


def moving_average(x, window_size=None):
    if window_size is None:
        window_size = len(x)//10
    return np.convolve(x, np.ones(window_size), 'valid') / window_size

plt.figure(figsize=(12, 8))
plt.plot(moving_average(rewards_without_partial_reward, 1000), label='No partial reward')
plt.plot(moving_average(rewards_with_partial_reward, 1000), label='Partial reward')
plt.plot(moving_average(rewards_random_policy_NoPArtial_reward, 1000), label='No partial reward (random policy)')
plt.plot(moving_average(rewards_random_policy_partial_reward, 1000), label='Partial reward (random policy)')
plt.xlabel('Episodes')
plt.ylabel('Reward')
#plt.plot(rewards, label='Reward')
plt.legend()
plt.show()





#!pip install gym
import gym
#import gymnasium as gym


env = gym.make("Blackjack-v1", sab=True)

action_size = env.action_space.n
state_size = env.observation_space
print(f"Number of states: {state_size}, Number of actions: {action_size}")


# again, hyperparameters that you can change
learning_rate = 0.01
n_training_episodes = 100000 # you can try also 1000000, it might take a while
epsilon = 0.1


state, _ = env.reset()
print(f"Initial state: {state}")





import random

def q_table(): #initialise the Q-table
    Qtable = {}
    for i in range(4, 22):  
        for j in range(1, 11): 
            for k in [True, False]:
                Qtable[(i, j, k)] = {0: 0.0, 1: 0.0}
    return Qtable
    
def greedy_policy(Qtable, state, usable_ace): #choose the action with the highest Q-value
    q_values =  Qtable[(state[0], state[1], usable_ace[2])]
    action = max(q_values, key=q_values.get)
    return int(action)

def epsilon_greedy_policy(Qtable, state, epsilon): #choose a random action with probability epsilon, otherwise choose the greedy action
    q_values =  Qtable[(state[0], state[1], state[2])]
    if np.random.rand() < epsilon:
        action = env.action_space.sample()
    else:
        action = greedy_policy(Qtable, state)
    return int(action)
    
def random_policy(): 
    return env.action_space.sample()


rewards = []
Qtable = q_table()
for episode in range(n_training_episodes):
    state, _ = env.reset()
    done = False
    cur_reward = 0
    while not done:
        action = epsilon_greedy_policy(Qtable, state, epsilon)
        new_state, reward, terminated, truncated, _ = env.step(action)
        cur_reward += reward
        Qtable[...] = ... 
        state = new_state
        done = terminated or truncated
        if done:
            rewards.append(cur_reward)
            break





# Plot the rewards
...





# Test the agent, get the average reward
...





#!pip install seaborn #uncomment if you haven't installed seaborn yet

from collections import defaultdict
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.patches import Patch

def create_grids(Qtable, usable_ace=False):
    """Create value and policy grid given an agent."""
    # convert our state-action values to state values
    # and build a policy dictionary that maps observations to actions
    state_value = defaultdict(float)
    policy = defaultdict(int)
    # If Qtable is an n-dimensional numpy array, we need to convert it to a dictionary
    if isinstance(Qtable, np.ndarray): #changes the Qtable to a dictionary if it is a numpy array
        Qtable = {k: v for k, v in np.ndenumerate(Qtable)}
    for obs, action_values in Qtable.items():
        state_value[obs] = float(np.max(action_values))
        policy[obs] = int(np.argmax(action_values))

    player_count, dealer_count = np.meshgrid(
        # players count, dealers face-up card
        np.arange(12, 22),
        np.arange(1, 11),
    )

    # create the value grid for plotting
    value = np.apply_along_axis(
        lambda obs: state_value[(obs[0], obs[1], usable_ace)],
        axis=2,
        arr=np.dstack([player_count, dealer_count]),
    )
    value_grid = player_count, dealer_count, value

    # create the policy grid for plotting
    policy_grid = np.apply_along_axis(
        lambda obs: policy[(obs[0], obs[1], usable_ace)],
        axis=2,
        arr=np.dstack([player_count, dealer_count]),
    )
    return value_grid, policy_grid


def create_plots(value_grid, policy_grid, title: str):
    #Creates a plot using a value and policy grid.
    # create a new figure with 2 subplots (left: state values, right: policy)
    player_count, dealer_count, value = value_grid
    fig = plt.figure(figsize=plt.figaspect(0.4))
    fig.suptitle(title, fontsize=16)

    # plot the state values
    ax1 = fig.add_subplot(1, 2, 1, projection="3d")
    ax1.plot_surface(
        player_count,
        dealer_count,
        value,
        rstride=1,
        cstride=1,
        cmap="viridis",
        edgecolor="none",
    )
    plt.xticks(range(12, 22), range(12, 22))
    plt.yticks(range(1, 11), ["A"] + list(range(2, 11)))
    ax1.set_title(f"State values: {title}")
    ax1.set_xlabel("Player sum")
    ax1.set_ylabel("Dealer showing")
    ax1.zaxis.set_rotate_label(False)
    ax1.set_zlabel("Value", fontsize=14, rotation=90)
    ax1.view_init(20, 220)

    # plot the policy
    fig.add_subplot(1, 2, 2)
    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap="Accent_r", cbar=False)
    ax2.set_title(f"Policy: {title}")
    ax2.set_xlabel("Player sum")
    ax2.set_ylabel("Dealer showing")
    ax2.set_xticklabels(range(12, 22))
    ax2.set_yticklabels(["A"] + list(range(2, 11)), fontsize=12)

    # add a legend
    legend_elements = [
        Patch(facecolor="lightgreen", edgecolor="black", label="Hit"),
        Patch(facecolor="grey", edgecolor="black", label="Stick"),
    ]
    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))
    return fig


# state values & policy with usable ace (ace counts as 11)
value_grid, policy_grid = create_grids(Qtable, usable_ace=True)
fig1 = create_plots(value_grid, policy_grid, title="With usable ace")
plt.show()


# state values & policy without usable ace (ace counts as 1)
value_grid, policy_grid = create_grids(Qtable, usable_ace=False)
fig2 = create_plots(value_grid, policy_grid, title="Without usable ace")
plt.show()



