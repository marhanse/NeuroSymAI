{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Reinforcement Learning\n",
    "\n",
    "In this assignment, we are going to first solve a simple RL exercise by reasoning about a 4-states simple MDP (Markov Decision Process), and afterwards implement a tabular RL agent that acts and (hopefully!) learns how to act in two different environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (3 points)\n",
    "\n",
    "Suppose you have an MDP with 4 states, numbered from 1 to 4. The possible transitions are: from state 1 to states 2 and 4, from 2 to 1 and 3, from 3 to 1 and 4. Each transition gets you a reward of -1, with 4 being the terminal state.\n",
    " \n",
    "Referring to the course slides, please answer to the questions in the markdown cells below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) A: Visualize the MDP (maybe write it down with pen and paper) and come up with the optimal policy (the best actions for each state). \n",
    "\n",
    "Answer: ...\n",
    "\n",
    "1) B: How would this change if from state 2, when taking action $a$ (moving from 2 to 1) we had a probability of 0.5 of remaining in state 2 and when taking action $b$ (moving from 2 to 3) we had a probability of 0.3 of remaining in state 2?\n",
    "\n",
    "Answer: ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) If now our policy $\\pi$ takes us from state 1 to 2, from 2 to 3, and from 3 to 4, what is $V_{\\pi}$?\n",
    "\n",
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Perform one step of synchronous value iteration, considering that $V_1(s)=2, \\forall s \\in S-\\{4\\}$, $V_1(4)=0$ (terminal state), $\\gamma = 0.99$, and for each state we have uniform probability over the action space.\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) How do different values of gamma impact RL?\n",
    "\n",
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) The agent starts in state 1 and selects the action to move to state 2. The reward for this transition is -1, and the Q-values are:  $Q(1,2) = -1.5$  and  $Q(1,4) = -0.5$. After arriving at state 2, the agent observes the Q-values:  $Q(2,1) = -2.0$  and  $Q(2,3) = -1.0$. Using a learning rate  $\\alpha = 0.1$  and  $\\gamma = 0.9$ , calculate the updated value of  $Q(1,2)$.\n",
    "\n",
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Describe the trade-off between exploration and exploitation in RL. How does epsilon-greedy attempt to tackle it? Under epsilon-greedy what is the probability of selecting the maximising action?\n",
    "\n",
    "Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (4 points)\n",
    "\n",
    "Now on to some RL implementation! This first exercise is in a custom environment which we will revisit again in Assignment 3. In this environment, your agent is a sous-chef tasked to prepare different kinds of pancakes. \n",
    "At first, we will keep it simple: the only ingredients your agent can play around with are pancake bread, bacon, and sauce, referred to with indices 0,1,2 respectively.\n",
    "\n",
    "The environment is given a set of recipes at the start, which will be the different goals the agent will randomly receive and try to reconstruct. Each goal is communicated to the agent at the start of an episode as a symbol (an index), which the agent does not have prior knowledge about: imagine that the head chef and your agent do not speak the same language, and that the head chef has left some different sketches for each recipe. Your agent will learn their meaning by interacting with the environment, preparing different pancakes and getting rewarded positively for getting the recipes right and negatively for getting them wrong.\n",
    "\n",
    "The observation your agent receives is the current pancake they have produced (e.g. [0, 1, 1] is a slice of pancake bread followed by two bacon strips), plus the symbol that represents one of the goals (e.g. 3 for goal 3, which the agent does not know anything about apriori).\n",
    "\n",
    "The actions your agent has at their disposal is to either position a pancake (0) a bacon strip (1) or some sauce (2).\n",
    "\n",
    "The episode terminates when the maximum size of the pancake is reached.\n",
    "\n",
    "At the start, the agent is then given a reward of +1 if it gets it right, -1 if it gets it wrong. This will change later!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = []\n",
    "recipes.append([0,1,2,0,1,2])\n",
    "recipes.append([0,1,1,0,1,1])\n",
    "recipes.append([0,1,1,0,2])\n",
    "recipes.append([0,2,0,2,0,2])\n",
    "recipes.append([0,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start, we will keep these goals fixed. Later we will ask you to try with different ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the environment for the pancake problem. \n",
    "The goal is to create a pancake with the same ingredients as the intended goal.\"\"\"\n",
    "\n",
    "class IngredientsActionSpace:\n",
    "    def __init__(self, n):\n",
    "        self.n = n  # Number of possible ingredients\n",
    "\n",
    "    def sample(self): #sample an action\n",
    "        return random.randint(0, self.n-1) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "class PancakeEnv:\n",
    "    def __init__(self, goals, max_pancake_size=6, partial_reward=False):\n",
    "        #initialise the environment\n",
    "        self.goals = goals #list of goals\n",
    "        self.max_pancake_size = max_pancake_size #maximum number of ingredients in a pancake (including the pancake itself)\n",
    "        self.action_space = IngredientsActionSpace(3) #0 is the pancake, 1 is the bacon, 2 is the sauce\n",
    "        self.observation_space() #create the observation space\n",
    "        self.partial_reward = partial_reward #if we want to give a partial reward\n",
    "\n",
    "\n",
    "    def reset(self, goal = None): #reset the environment, choose a random goal if none is given\n",
    "        self.word_for_goal = random.randint(0, len(self.goals)-1) if goal is None else goal #choose a random goal (note that we pass an index, not the actual goal pancake!)\n",
    "        self.intended_goal = self.goals[self.word_for_goal] #get the actual goal pancake (we won't give it to the agent, but use it for computing the reward)\n",
    "        self.current_pancake = [0] #we always start with a pancake with no ingredients\n",
    "        return self.word_for_goal, self.current_pancake\n",
    "\n",
    "    def step(self, action): #act 1 step in the environment\n",
    "        self.current_pancake.append(action) #add the ingredient to the pancake\n",
    "        reward, done = self.compute_reward_and_done() #compute the reward and if the episode is done\n",
    "        return self.current_pancake, reward, done\n",
    "        \n",
    "    def compute_reward_and_done(self):\n",
    "        if self.current_pancake == self.intended_goal: #we have reached the goal\n",
    "                return 1, True\n",
    "        if len(self.current_pancake) == self.max_pancake_size: # we have reached the maximum number of steps\n",
    "            #if we want to give a partial reward, we compute it here, otherwise we return 0 (failure)\n",
    "            return 0 if not self.partial_reward else partial_reward(self.intended_goal, self.current_pancake), True\n",
    "        return 0, False #we are not done yet\n",
    "        \n",
    "    \n",
    "    def observation_space(self):\n",
    "        # We have all the permutations of pancakes ingredients, from a length of 1 to max_pancake_size. list them all\n",
    "        # e.g. for max_pancake_size=3, we have\n",
    "        # [0] [0,0] [0,1] [0,2] [0,0,0] [0,0,1] [0,0,2], [0,1,0]...\n",
    "        self.observation_space = []\n",
    "        for i in range(1, self.max_pancake_size+1):\n",
    "            self.observation_space += self.permutations(i)\n",
    "    \n",
    "    def permutations(self, n): #recursive function to get all the permutations of pancakes ingredients\n",
    "        if n == 1:\n",
    "            return [[0]]\n",
    "        perms = []\n",
    "        for perm in self.permutations(n-1):\n",
    "            for i in range(3):\n",
    "                perms.append(perm + [i])\n",
    "        return perms\n",
    "    \n",
    "    def get_observation_index(self, current_pancake): #get the index of the current pancake in the observation space (used for indexing the Q-table)\n",
    "        return self.observation_space.index(current_pancake)\n",
    "    \n",
    "def partial_reward(goal, current_pancake):\n",
    "    # we give a partial reward based on the number of ingredients that at the right place\n",
    "    reward = 0\n",
    "    for i in range(len(goal)):\n",
    "        if goal[i] == current_pancake[i]:\n",
    "            reward += 1\n",
    "    return reward/len(goal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we start without partial reward, so our agent will be rewarded ONLY if the agent is able to reconstruct the whole pancake. This makes this simple reward actually very hard to learn, as we will see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, [0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = PancakeEnv(recipes, max_pancake_size=6, partial_reward=False)\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How large is the observation space? And the goal space? What about the action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Observation space size: \", len(env. ...))\n",
    "print(\"Goal space size: \", len(env. ...))\n",
    "print(\"Action space size: \", len(env. ...))\n",
    "print(\"Sample observation: \", env. ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to sample a starting goal and observation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-Learning is a simple RL model-free algorithm which you have covered in the lecture. \n",
    "\n",
    "We have included the algorithm below. Can you implement it? We have implemented some helper functions but you should fill in the rest.\n",
    "\n",
    "<!-- ![text](algorithm.png) -->\n",
    "[![Q-learning algorithm](https://miro.medium.com/v2/resize:fit:1400/1*7AWfjw8YDfoRqnIO71DjiA.png)](https://en.wikipedia.org/wiki/Q-learning)\n",
    "\n",
    "Note that in this case, actions will have different consequences depending on the goal we are aiming to achieve. This means that we cannot have one single Q-table for each goal, or our updates will be conflicting. The first dimension of the Q-table should be the number of goals!\n",
    "\n",
    "In reality, the goal symbol is actually part of the state space. We keep it separate from the observation space though as it will be easier to index both with what the environment returns us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_table(): #initialise the Q-table\n",
    "  Qtable = np.zeros(...)\n",
    "  return Qtable\n",
    "    \n",
    "def greedy_policy(Qtable, ...): #choose the action with the highest Q-value\n",
    "  action = ...\n",
    "  return int(action)\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, ...): #choose a random action with probability epsilon, otherwise choose the greedy action\n",
    "    ...\n",
    "    \n",
    "def random_policy(): \n",
    "    return env.action_space.sample()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide some parameters for you. Feel free to play around with them and see how good your agent gets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 100000  # Total training episodes\n",
    "max_steps = 5 # DO NOT CHANGE: Max steps per episode (ingredients you can place)\n",
    "learning_rate = 1          # Learning rate\n",
    "\n",
    "# Environment parameters\n",
    "gamma = 0.99                # Discounting rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to experiment with different $\\epsilon$ , and see how much exploration is required to solve this problem. \n",
    "A good idea is always to also try a decaying epsilon when fixed epsilons are not doing great, granting more exploration at the start ($\\epsilon$ close to 1), and more exploitation towards the end ($\\epsilon$ close to 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Optional: implement a function to decay epsilon over time and plot it\"\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "Qtable = q_table(...)\n",
    "for episode in range(n_training_episodes):\n",
    "    goal, state = env.reset()\n",
    "    done = False\n",
    "    for step_n in range(max_steps):\n",
    "        state_idx = env.get_observation_index(state)\n",
    "        action = ...\n",
    "        new_state, reward, done = env.step(action)\n",
    "        new_state_idx = env.get_observation_index(new_state)\n",
    "        Qtable[...] = ...\n",
    "        state = new_state\n",
    "        if done:\n",
    "            print(f\"Episode {episode}, reward {reward}\")\n",
    "            rewards.append(reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Plot the rewards - depending on your exploration strategy, the curve might be hard to interpret,\" \n",
    "\"try using a moving average to smooth it out\" \n",
    "\n",
    "def moving_average(x, window_size=None):\n",
    "    if window_size is None:\n",
    "        window_size = len(x)//10\n",
    "    return np.convolve(x, np.ones(window_size), 'valid') / window_size\n",
    "\n",
    "#plt.plot(moving_average(rewards, 1000))\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not worry if you don't manage to get an agent that has perfectly learnt the environment, try to obtain the best reward plot you can get. \n",
    "\n",
    "We are going to try change the reward strategy of the environment, meaning that now our agent receives a partial reward at the end of each episode, i.e. the fraction of ingredients that match with the actual ingredient of the recipe.\n",
    "\n",
    "Try again to train a new Q-learning agent and obtain the rewards plot. Compare the previous plot and this one qualitatively!\n",
    "\n",
    "*(Optional)* For both environments (with and without partial reward) it can help to be aware of how a random agent would perform on the environment, just to have a very low baseline, to understand if our agent is learning anything. Feel free to use the random_policy() function and plot how the rewards look for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PancakeEnv(recipes, max_pancake_size=6, partial_reward=True)\n",
    "\n",
    "\"\"\"Your code here\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 3: Blackjack! (3 points)\n",
    "\n",
    "For those that are unfamiliar, blackjack is a common card game. There are (at least) two-agents, the dealer and the player. In our case, at the beginning of the game the dealer gives our agent one card. The dealer's card is placed face up. The dealer also gets two cards: one is face up, the other face down. \n",
    "The player observes their own card and the dealer's card and must decide to 'hit' (i.e. get a new card) or to 'stick'(i.e. keep the hand that they've got). \n",
    "\n",
    "If the player selects 'stick' it is then the dealers turn, which will turn the hidden card face-up, and then hit until their sum is at least or they bust (they go over 21). The goal is to beat the dealer, i.e. get a higher sum than them without busting ourselves.\n",
    "\n",
    "This environment comes from the popular gymnasium library, which has a lot of different RL environments for you to try. You can read more info about this environment and the rules of blackjack it follows at https://gymnasium.farama.org/environments/toy_text/blackjack/.\n",
    "\n",
    "Here are the main points:\n",
    "\n",
    "**Action Space**\n",
    "\n",
    "There are two actions: stick (0), and hit (1).\n",
    "\n",
    "**Observation Space**\n",
    "\n",
    "The observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1).\n",
    "\n",
    "**Rewards**\n",
    "\n",
    "win game: +1\n",
    "\n",
    "lose game: -1\n",
    "\n",
    "draw game: 0\n",
    "\n",
    "\n",
    "[![Blackjack!](https://www.gymlibrary.dev/_images/blackjack.gif)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym #uncomment if you haven't installed gym yet\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space\n",
    "print(f\"Number of states: {state_size}, Number of actions: {action_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, hyperparameters that you can change\n",
    "learning_rate = 0.01\n",
    "n_training_episodes = 100000 # you can try also 1000000, it might take a while\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "print(f\"Initial state: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above how the observation looks. As we said before, the states are tuples of 3 values. Modify the functions defining the Q-table and your policies accordingly (if needed), keeping in mind we will need to store 2 q-values for each combination of these values, since we have two actions.\n",
    "\n",
    "P.S. Note that you can reuse a lot of code from Part 2, and you are also encouraged to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def q_table(): #initialise the Q-table\n",
    "  Qtable = ...\n",
    "  return Qtable\n",
    "    \n",
    "def greedy_policy(...): #choose the action with the highest Q-value\n",
    "  action = ...\n",
    "  return int(action)\n",
    "\n",
    "def epsilon_greedy_policy(...): #choose a random action with probability epsilon, otherwise choose the greedy action\n",
    "    ...\n",
    "    \n",
    "def random_policy():\n",
    "    return env.action_space.sample()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for episode in range(n_training_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    cur_reward = 0\n",
    "    while not done:\n",
    "        action = ...\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        cur_reward += ...\n",
    "        Qtable[...] = ... \n",
    "        state = new_state\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            rewards.append(cur_reward)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the rewards and see if the agent is learning something. Again, it might be a good idea to plot a moving average of your rewards, as depending on your exploration tactic the reward might be very noisy.\n",
    "\n",
    "What result do you get? Does it seem reasonable given our expectations knowing the blackjack game (the dealer always have the advantage)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rewards\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see how good our agent is! Try to see what the average reward your agent gets in 100 episodes.\n",
    "(What is the difference from the training loop above? Since we are now testing, do we want to update the Qtable? Also, what is the policy we want to use now?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent, get the average reward\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the agent's policy\n",
    "\n",
    "Note that this code works with the Q-table being represented as a dictionary with keys being the observations and values being numpy arrays of length 2 (one element for each action). If you have represented your Q-table as an n-dimensional array, you might have to modify the create_grids function (or change the Q-table representation to a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn #uncomment if you haven't installed seaborn yet\n",
    "\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def create_grids(Qtable, usable_ace=False):\n",
    "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
    "    # convert our state-action values to state values\n",
    "    # and build a policy dictionary that maps observations to actions\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    # If Qtable is an n-dimensional numpy array, we need to convert it to a dictionary\n",
    "    if isinstance(Qtable, np.ndarray): #changes the Qtable to a dictionary if it is a numpy array\n",
    "        Qtable = {k: v for k, v in np.ndenumerate(Qtable)}\n",
    "    for obs, action_values in Qtable.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # players count, dealers face-up card\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11),\n",
    "    )\n",
    "\n",
    "    # create the value grid for plotting\n",
    "    value = np.apply_along_axis(\n",
    "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    value_grid = player_count, dealer_count, value\n",
    "\n",
    "    # create the policy grid for plotting\n",
    "    policy_grid = np.apply_along_axis(\n",
    "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "        axis=2,\n",
    "        arr=np.dstack([player_count, dealer_count]),\n",
    "    )\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    #Creates a plot using a value and policy grid.\n",
    "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # plot the state values\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(\n",
    "        player_count,\n",
    "        dealer_count,\n",
    "        value,\n",
    "        rstride=1,\n",
    "        cstride=1,\n",
    "        cmap=\"viridis\",\n",
    "        edgecolor=\"none\",\n",
    "    )\n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
    "    ax1.set_title(f\"State values: {title}\")\n",
    "    ax1.set_xlabel(\"Player sum\")\n",
    "    ax1.set_ylabel(\"Dealer showing\")\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "\n",
    "    # plot the policy\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
    "    ax2.set_title(f\"Policy: {title}\")\n",
    "    ax2.set_xlabel(\"Player sum\")\n",
    "    ax2.set_ylabel(\"Dealer showing\")\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
    "\n",
    "    # add a legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
    "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
    "    ]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# state values & policy with usable ace (ace counts as 11)\n",
    "value_grid, policy_grid = create_grids(Qtable, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state values & policy without usable ace (ace counts as 1)\n",
    "value_grid, policy_grid = create_grids(Qtable, usable_ace=False)\n",
    "fig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see some patterns in the value function and policy plots of your agents? What is the difference between when we have a usable ace or not at the start? Comment below:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
