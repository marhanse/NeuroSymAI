{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5fb54750-8f18-44e4-a9fd-a0f151b51404",
      "metadata": {
        "id": "5fb54750-8f18-44e4-a9fd-a0f151b51404"
      },
      "source": [
        "# Assignment 3 - Emergent Communication and Abstractions?\n",
        "\n",
        "In this assignment we are going to explore the field of Emergent Communication and see how it situates within Cognitive Science as a tool to explain how language is shaped and evolves.\n",
        "\n",
        "We will first explore how emergent communication works and how two agents can interact with each other to collaboratively solve a task via communicating together. We will then explore how the mechanism of abstractions learning can be used in tandem with emergent communication to obtain results that mimic how humans tend towards concisiveness while playing a collaborative game over repeated interactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4966f6f-b3ba-455e-845a-cf37deac20a9",
      "metadata": {
        "id": "c4966f6f-b3ba-455e-845a-cf37deac20a9"
      },
      "source": [
        "# Part 1: Emergent Communication (5 points)\n",
        "\n",
        "Emergent Communication is an area of research within Deep Learning. It studies how populations of agents (in ours and many other cases 2 agents) develop language in order to solve cooperative tasks. Motivations for study in this area include deriving better algorithms for enabling inter-agent cooperation through communication and understanding why human languages have the qualities which they do. This is usually studied within Lewis signalling games, which we will introduce now.\n",
        "\n",
        "![A Lewis signalling game example.](https://robertodessi.github.io/images/focus.png)\n",
        "\n",
        "A lewis signalling game is a 2-player cooperative game. The two agents are referred to as the speaker and the listener. The speaker receives an observation (e.g. image, feature vector, etc.) and constructs a message which is communicated to the listener. The listener receives the message and a set of observations. One of these observations is the one that the speaker observed. Using the message and the observation, the listener selects the observation which they think is the best match.\n",
        "\n",
        "The emergent communication part of this assignment will proceed as follows:\n",
        "1) Program a Q-learning algorithm which will be used by both the speaker and the listener\n",
        "2) Train and evaluate the agents on a simple dataset\n",
        "\n",
        "\n",
        "## Define Q-Learning for Speaker and Listener\n",
        "\n",
        "In Emergent Communication, our models for both agents are usually neural networks. This is because neural networks allow for great generalization and have a bias for simplicity, making them really helpful at analyzing language evolution. Here instead, to keep things simple, we will use tabular Q-learning for our agents like in Assignment 1. Differently from Assignment 1, we now have 2 (!) agents.\n",
        "\n",
        "This means we will have 2 Q-tables, one that our first agent (the speaker) will use to choose the word to communicate given the observation, and one that our second agent (the listener) will use to choose what action to pick given the word the speaker communicates to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e2da6a-5d74-49e1-a63f-bbf41b939674",
      "metadata": {
        "id": "d3e2da6a-5d74-49e1-a63f-bbf41b939674"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c5d96b",
      "metadata": {
        "id": "75c5d96b"
      },
      "outputs": [],
      "source": [
        "\"Some useful functions\"\n",
        "\n",
        "def reward_function(actual, guess):\n",
        "    return int(actual == guess)\n",
        "\n",
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "def visualise_policies(speaker, listener, num_concepts, preamble = ''):\n",
        "    if preamble:\n",
        "        print(preamble)\n",
        "    for c in range(num_concepts):\n",
        "        message = speaker.sample_action(c, test=True)\n",
        "        action = listener.sample_action(message, test=True)\n",
        "        reward = reward_function(c, action)\n",
        "        print('Concept: %i, Message: %i, Action: %i, Reward: %i'%(c, message, action, reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbb9b23",
      "metadata": {
        "id": "3bbb9b23"
      },
      "source": [
        "Instantiate two Q-tables, one for the Speaker and one for the Listener. Make sure that the Speaker maps from concepts to messages and the listener from messages to concepts.\n",
        "\n",
        "We will start by having 5 concepts and 5 messages, so the Speaker will try to instruct the Listener about 5 different objects. Note that in Emergent Communication usually the episode lasts only one step, so the Q-learning update is simpler than usual (called myopic). The update thus does not involve estimating future rewards. You can achieve that by just simplifying the Q-learning update rule with $\\gamma = 0$.\n",
        "\n",
        "<!-- ![text](algorithm.png) -->\n",
        "[![Q-learning algorithm](https://miro.medium.com/v2/resize:fit:1400/1*7AWfjw8YDfoRqnIO71DjiA.png)](https://en.wikipedia.org/wiki/Q-learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128ea289",
      "metadata": {
        "id": "128ea289"
      },
      "outputs": [],
      "source": [
        "\"Your code here\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5768c7b9",
      "metadata": {
        "id": "5768c7b9"
      },
      "source": [
        "Complete the training loop. Make sure to update both the speaker and the listener. Usually the learning rate is the same but good reasons can be made to do it differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf23420-efb6-44fe-ad8a-942f0d1e5392",
      "metadata": {
        "id": "5cf23420-efb6-44fe-ad8a-942f0d1e5392"
      },
      "outputs": [],
      "source": [
        "# Speaker\n",
        "num_concepts = 5\n",
        "num_messages = 5\n",
        "n_iterations = 100000\n",
        "#speaker\n",
        "speaker = ...\n",
        "\n",
        "# Listener\n",
        "listener = ...\n",
        "\n",
        "# Training\n",
        "rew_list = []\n",
        "for epoch in range(100000):\n",
        "    concept = np.random.randint(0, num_concepts)\n",
        "    message = ...\n",
        "    action = ...\n",
        "    reward = reward_function(concept, action)\n",
        "    \"Updates\"\n",
        "    ... #speaker\n",
        "    ... #listener\n",
        "    rew_list.append(reward)\n",
        "\n",
        "\n",
        "plt.plot(moving_average(rew_list, 500))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment here what choices you ended up taking to make communication converge to a good reward:\n",
        "\n",
        "Your answer: ..."
      ],
      "metadata": {
        "id": "VrsOHpEK76XD"
      },
      "id": "VrsOHpEK76XD"
    },
    {
      "cell_type": "markdown",
      "id": "a8133e1b-a1fe-48a1-94b8-56bf07ff644b",
      "metadata": {
        "id": "a8133e1b-a1fe-48a1-94b8-56bf07ff644b"
      },
      "source": [
        "## Part 2: Pancake stacks, a more complex dataset (5 points)\n",
        "\n",
        "We are going to consider a simplification of the tower-building environment that is presented in McCarthy (https://arxiv.org/pdf/2107.00077). Instead, we are going to consider an agent that creates pancake stacks.\n",
        "\n",
        "The environment is a simplification of the one of Assignment 1: instead of receiving reward only at the end of the episode, the speaker and listener get rewarded at each step for getting the ingredients right or wrong.\n",
        "\n",
        "You can imagine it as if the speaker is able to see  at each step what ingredient the listener uses and can tell them if they are doing the right or wrong thing, meanwhile the speaker itself can implicitly reward itself for picking the word that made the listener pick the right action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ef7463-c329-4510-9d85-b5d82a4c3ea7",
      "metadata": {
        "id": "88ef7463-c329-4510-9d85-b5d82a4c3ea7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sequences = []\n",
        "\n",
        "    def add_sequence(self, seq):\n",
        "        self.sequences.append(seq)\n",
        "\n",
        "    def get_dataset(self):\n",
        "        return self.sequences\n",
        "\n",
        "    def get_average_length(self):\n",
        "        lengths = [len(s) for s in self.sequences]\n",
        "        return sum(lengths)/len(lengths)\n",
        "\n",
        "    def rewrite(self, old_subseq, new_subseq):\n",
        "        total_reductions = 0\n",
        "        for seq in self.sequences:\n",
        "            n = len(old_subseq)\n",
        "            for i in range(len(seq) - n + 1):\n",
        "                if seq[i:i+n] == old_subseq:\n",
        "                    seq[i:i+n] = new_subseq\n",
        "                    total_reductions += 1\n",
        "        return total_reductions\n",
        "\n",
        "\n",
        "def my_dataset():\n",
        "    recipes = Dataset()\n",
        "    recipes.add_sequence([0,1,2,0,1,2])\n",
        "    recipes.add_sequence([0,1,1,0,1,1])\n",
        "    recipes.add_sequence([0,1,1,0,2])\n",
        "    recipes.add_sequence([0,2,0,2,0,2])\n",
        "    recipes.add_sequence([0,1,1,1,1,1])\n",
        "    recipes.add_sequence([0,2,2,2,2,2])\n",
        "    recipes.num_ingredients = 3\n",
        "    return recipes\n",
        "\n",
        "recipes = my_dataset()\n",
        "\n",
        "# An example of a rewrite.\n",
        "#recipes.rewrite([1,1],[3])\n",
        "#recipes.sequences\n",
        "\n",
        "def sample_goal(dataset):\n",
        "     return random.choice(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f745b5",
      "metadata": {
        "id": "64f745b5"
      },
      "source": [
        "First of all, we try a normal communication game where the speaker samples a recipe, and tries to communicate step by step to the listener, who has to recreate the pancake the speaker intended to communicate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e024d5",
      "metadata": {
        "id": "98e024d5"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "recipes = my_dataset()\n",
        "\n",
        "# Speaker\n",
        "num_concepts = 3 # Maximum number of ingredients and abstractions\n",
        "num_messages = 5 # Maximum number of messages\n",
        "n_iterations = 100000\n",
        "speaker = ...\n",
        "\n",
        "# Listener\n",
        "listener = ...\n",
        "\n",
        "# Training\n",
        "rew_list = []\n",
        "dataset = recipes.get_dataset()\n",
        "visualise_policies(speaker, listener, num_concepts, preamble='Before training loop')\n",
        "for epoch in range(n_iterations):\n",
        "    goal = sample_goal(dataset) #Sample a recipe from the dataset\n",
        "    ep_rewards = 0\n",
        "    for concept in goal:\n",
        "        message = ...\n",
        "        action = ...\n",
        "        reward = ...\n",
        "        \"Updates\"\n",
        "        ... #speaker\n",
        "        ... #listener\n",
        "        ep_rewards += reward\n",
        "    rew_list.append(ep_rewards/len(goal))\n",
        "\n",
        "visualise_policies(speaker, listener, num_concepts, preamble='After training loop')\n",
        "\n",
        "plt.plot(moving_average(rew_list, 1000))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e51e4e9d",
      "metadata": {
        "id": "e51e4e9d"
      },
      "source": [
        "Hopefully communication works! If we consider the length of an utterance, here defined as the whole sentence needed to describe a full scene (a goal pancake), you can easily count how long a round of communication is on average.\n",
        "\n",
        "Your answer:\n",
        "\n",
        "$\\frac{\\sum_{goal_i}len(goal_i)}{n_{goals}} = ...$\n",
        "\n",
        "In the McCarthy paper above, it is shown that humans tend to more concise utterances the more they collaborate on the task. This is achieved through the introduction of abstractions.\n",
        "\n",
        "In our case an abstraction might look like \"a double stack of bacon\" or \"two bacon stripes with sauce in between\". Whatever appears frequently in the dataset is more likely to be \"abstracted\" into a concept carrying more information!\n",
        "\n",
        "Thus, we will introduce a mechanism of introducing abstractions in Emergent Communication, to obtain a two-agents system that show the same tendencies as the ones in humans from the McCarthy paper linked above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46ff7a7",
      "metadata": {
        "id": "d46ff7a7"
      },
      "source": [
        "We will intertwine the communication with an Abstraction phase, where the Speaker can revise their experience and find the most common subsequence. It can then introduce a new concept (macro-ingredient in our case): an abstraction!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c617cf6",
      "metadata": {
        "id": "2c617cf6"
      },
      "outputs": [],
      "source": [
        "def find_abstractions(dataset):\n",
        "    \"We want to find the most common subsequences in the dataset\"\n",
        "    all_subsequences = {}\n",
        "    for seq in dataset:\n",
        "        \"Your code: count the frequency of all subsequences of all lengths\"\n",
        "        ...\n",
        "    \"Your code: return the most common subsequence, if there are ties, return one of them at random\"\n",
        "    ...\n",
        "    return ... #e.g. if you find [1,1] to be the most common subsequence, return [1,1]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecf8f8f9-20c4-43ec-a1cf-e3029b004678",
      "metadata": {
        "id": "ecf8f8f9-20c4-43ec-a1cf-e3029b004678"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "recipes = my_dataset()\n",
        "\n",
        "# Speaker\n",
        "\"We will use 2 Q-learning agents again\"\n",
        "num_concepts = 8 # Maximum number of ingredients and abstractions (at first, the q-values at the abstraction indexes will not have a meaning but as soon we introduce a new abstraction, one more will)\n",
        "num_messages = 8 # Maximum number of messages (one per concept we want to communicate)\n",
        "speaker = ...\n",
        "listener = ...\n",
        "\n",
        "\n",
        "\"The number of abstractions phases we will do, i.e. how many times we will find an abstraction\"\n",
        "n_abstractions = 5\n",
        "\n",
        "starting_concepts = recipes.num_ingredients\n",
        "\n",
        "# Training\n",
        "rew_list = []\n",
        "utterance_length_list = []\n",
        "for step in range(n_abstractions):\n",
        "    dataset = recipes.get_dataset()\n",
        "    utterance_length_list.append(recipes.get_average_length())\n",
        "    for epoch in range(n_iterations):\n",
        "        goal = sample_goal(dataset) #Sample a recipe from the dataset\n",
        "        ep_rewards = 0\n",
        "        for concept in goal:\n",
        "            message = ...\n",
        "            action = ...\n",
        "            reward = ...\n",
        "            \"Updates\"\n",
        "            ... #speaker\n",
        "            ... #listener\n",
        "            ep_rewards += reward\n",
        "        rew_list.append(ep_rewards/len(goal))\n",
        "    visualise_policies(speaker, listener, starting_concepts + step, preamble=f'After training loop ({n_abstractions} abstractions)')\n",
        "    abstraction = find_abstractions(dataset)\n",
        "    reductions = recipes.rewrite(abstraction, [starting_concepts + step])\n",
        "\n",
        "    print(f'Abstraction: {abstraction}, Reductions: {reductions}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(moving_average(rew_list, 1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c0b049",
      "metadata": {
        "id": "24c0b049"
      },
      "source": [
        "Communication should always go back up to 100% correct!\n",
        "\n",
        "What about utterance length? Does it decrease as it does in humans?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086c72b6-9cf9-424a-94c6-16705c71e3fc",
      "metadata": {
        "id": "086c72b6-9cf9-424a-94c6-16705c71e3fc"
      },
      "outputs": [],
      "source": [
        "# Plot the average length of the recipes\n",
        "\n",
        "plt.plot(utterance_length_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b288f09e",
      "metadata": {
        "id": "b288f09e"
      },
      "source": [
        "Hopefully this gave you a simple idea of how Emergent Communication work and why we found that abstractions learning can be used with it to recreate some human tendencies in repeated communication."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}